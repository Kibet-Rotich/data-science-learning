{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üå± Introduction to a Simple Machine Learning Pipeline\n",
    "\n",
    "In this notebook, we'll go through a simple example of how a **machine learning pipeline** works ‚Äî from loading data, to training a model, to making predictions and evaluating performance.\n",
    "\n",
    "We'll use the **Iris dataset**, one of the most popular beginner datasets in machine learning. It contains measurements of flower petals and sepals, and our task is to classify them into three species: *Setosa*, *Versicolor*, and *Virginica*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Loading the Data\n",
    "\n",
    "We‚Äôll start by loading the dataset using `scikit-learn`, a popular machine learning library. The dataset includes:\n",
    "- `data`: numerical measurements of each flower (features)\n",
    "- `target`: the class labels (which species)\n",
    "- `feature_names`: names of the features\n",
    "- `target_names`: names of the flower species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "print(iris.data[:5])  # show first 5 rows\n",
    "print(iris.feature_names)\n",
    "print(iris.target[:10])\n",
    "print(iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Splitting the Data ‚Äî Training vs Testing\n",
    "\n",
    "Before training, we split our dataset into **training** and **testing** sets:\n",
    "- **Training data** is used to teach the model.\n",
    "- **Testing data** is used to check how well the model performs on unseen data.\n",
    "\n",
    "We'll use an 80/20 split ‚Äî meaning 80% for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris.data, iris.target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print('Training data shape:', X_train.shape)\n",
    "print('Testing data shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Building and Training the Model\n",
    "\n",
    "We'll use a simple **K-Nearest Neighbors (KNN)** classifier.\n",
    "\n",
    "**How it works:**\n",
    "- When given a new data point, KNN looks at the *k* nearest points in the training data.\n",
    "- It predicts the most common label among those neighbors.\n",
    "\n",
    "It‚Äôs a good first model because it‚Äôs simple and intuitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Making Predictions\n",
    "\n",
    "After training, the model can make predictions on the **test data**. Let‚Äôs see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print('Predictions:', y_pred)\n",
    "print('Actual:', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Evaluating the Model\n",
    "\n",
    "Now we‚Äôll check how accurate our model is. The simplest metric is **accuracy**, which measures the percentage of correct predictions.\n",
    "\n",
    "We can also use a **classification report** for a deeper look at performance ‚Äî it includes:\n",
    "- **Precision:** Of all predicted positives, how many were correct.\n",
    "- **Recall:** Of all actual positives, how many were identified correctly.\n",
    "- **F1-score:** A balance between precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Model Accuracy:', accuracy)\n",
    "\n",
    "print('\\nDetailed Report:')\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "In this notebook, we went through the complete **machine learning workflow**:\n",
    "1. Load and explore data\n",
    "2. Split data into training and testing sets\n",
    "3. Build and train a model (KNN)\n",
    "4. Make predictions\n",
    "5. Evaluate results\n",
    "\n",
    "This process forms the foundation for all ML projects ‚Äî whether simple models like KNN or advanced ones like Neural Networks. üöÄ"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
